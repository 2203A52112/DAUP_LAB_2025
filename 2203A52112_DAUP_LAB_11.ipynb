{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRgSiWhVhrs3jkneZFwjBp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2203A52112/DAUP_LAB_2025/blob/main/2203A52112_DAUP_LAB_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtHarQ3DnzS2",
        "outputId": "1010e268-d8b7-46a5-9960-300acae63edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance:\n",
            "Logistic Regression: {'Accuracy': 0.9423076923076923, 'Precision': 0.9833333333333333, 'Recall': 0.921875, 'False Positive Rate': np.float64(0.025), 'False Negative Rate': np.float64(0.078125)}\n",
            "Decision Tree: {'Accuracy': 0.9903846153846154, 'Precision': 1.0, 'Recall': 0.984375, 'False Positive Rate': np.float64(0.0), 'False Negative Rate': np.float64(0.015625)}\n",
            "Random Forest: {'Accuracy': 0.9903846153846154, 'Precision': 1.0, 'Recall': 0.984375, 'False Positive Rate': np.float64(0.0), 'False Negative Rate': np.float64(0.015625)}\n",
            "Gradient Boosting: {'Accuracy': 0.9903846153846154, 'Precision': 1.0, 'Recall': 0.984375, 'False Positive Rate': np.float64(0.0), 'False Negative Rate': np.float64(0.015625)}\n",
            "SVM: {'Accuracy': 0.9326923076923077, 'Precision': 0.9830508474576272, 'Recall': 0.90625, 'False Positive Rate': np.float64(0.025), 'False Negative Rate': np.float64(0.09375)}\n",
            "KNN: {'Accuracy': 0.9423076923076923, 'Precision': 0.9833333333333333, 'Recall': 0.921875, 'False Positive Rate': np.float64(0.025), 'False Negative Rate': np.float64(0.078125)}\n",
            "\n",
            "Z-Test on False Negative Rates:\n",
            "{'Z-Statistic': np.float64(-0.40217140021108533), 'P-Value': np.float64(0.6875578886678757)}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from scipy.stats import norm, ttest_ind\n",
        "# Import matplotlib.pyplot and seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/diabetes_data_upload.csv\")\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in df.columns:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=[\"class\"])\n",
        "y = df[\"class\"]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel=\"linear\", probability=True, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "fnr_results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"False Positive Rate\": fp / (fp + tn),\n",
        "        \"False Negative Rate\": fn / (fn + tp)\n",
        "    }\n",
        "    fnr_results[name] = fn / (fn + tp)\n",
        "\n",
        "# Perform Z-Test on FNR differences between two models\n",
        "def perform_z_test(model1, model2):\n",
        "    fnr1, fnr2 = fnr_results[model1], fnr_results[model2]\n",
        "    n1, n2 = len(y_test), len(y_test)\n",
        "\n",
        "    std_error = np.sqrt((fnr1 * (1 - fnr1) / n1) + (fnr2 * (1 - fnr2) / n2))\n",
        "    z_stat = (fnr1 - fnr2) / std_error\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "    return {\"Z-Statistic\": z_stat, \"P-Value\": p_value}\n",
        "\n",
        "z_test_fnr = perform_z_test(\"Logistic Regression\", \"SVM\")\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"{model}: {metrics}\")\n",
        "\n",
        "print(\"\\nZ-Test on False Negative Rates:\")\n",
        "print(z_test_fnr)\n"
      ]
    }
  ]
}